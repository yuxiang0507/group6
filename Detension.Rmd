---
title: "返校"
author: "Louis Zheng"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
---
# 系統設置

```{r}
Sys.setlocale(category = "LC_ALL", locale = "zh_TW.UTF-8") # For ubuntu
Sys.setlocale("LC_CTYPE", "cht") # For windows.
```

## 安裝需要的packages
```{r message=FALSE, warning=FALSE}
packages = c("readr","tm", "data.table", "dplyr", "stringr", "jiebaR", "tidytext", "ggplot2", "tidyr", "topicmodels", "LDAvis", "webshot","purrr","ramify","RColorBrewer", "htmlwidgets","servr")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
```

```{r  message=FALSE, warning=FALSE}
require(readr)
library(readr)
require(tm)
require(data.table)
require(dplyr)
require(stringr)
require(jiebaR)
require(udpipe)
require(tidytext)
require(ggplot2)
require(tidyr)
require(topicmodels)
require(LDAvis)
require(wordcloud2)
require(webshot)
require(htmlwidgets)
require(servr)
require(purrr)
require(ramify)
require(RColorBrewer)
mycolors <- colorRampPalette(brewer.pal(8, "Set3"))(20)
```

# 讀取資料
## 載入資料

```{r message=FALSE, warning=FALSE, echo=FALSE}
Sys.setlocale(category = "LC_ALL", locale = "zh_TW.UTF-8") # 避免中文亂碼
```

```{r}
Detention <- fread("Detention2.csv", encoding = "UTF-8")
Detention$artDate = Detention$artDate %>% as.Date("%Y/%m/%d") # 將日期欄位格式由chr轉為dat
```

### 資料描述

+ 透過中山管院文字分析平台在PTT八卦版和電影版，搜尋關鍵字：[返校]，時間從2019-08-01~2020-05-10，總共613篇

```{r}
Detention %>% 
  group_by(artDate) %>%
  summarise(count = n())%>%
  ggplot(aes(artDate,count))+
    geom_line(color="red")+
  geom_point()
```

> 可觀察資料主要分佈在：去年9月返校電影上映後之後


## Tokenization 
```{r}
# 使用默認參數初始化一個斷詞引擎
jieba_tokenizer = worker()
Detention_tokenizer <- function(t) {
  lapply(t, function(x) {
    if(nchar(x)>1){
      tokens <- segment(x, jieba_tokenizer)
      stop_words <- c("可以","一個","沒有","覺得","我們","因為","就是","什麼")
      tokens <- filter_segment(tokens, stop_words)
      # 去掉字串長度爲1的詞彙
      tokens <- tokens[nchar(tokens)>1]
      return(tokens)
    }
  })
}
```

```{r}
tokens <- Detention %>%
  unnest_tokens(word, sentence, token=Detention_tokenizer) %>%
  filter(!str_detect(word, regex("[0-9a-zA-Z]"))) %>%
  count(artUrl, word) %>%
  rename(count=n)
tokens %>% head(20)
```

## 將資料轉換為Document Term Matrix (DTM)
```{r}
Detention_dtm <- tokens %>% cast_dtm(artUrl, word, count)
Detention_dtm
inspect(Detention_dtm[1:10,1:10])
```

> 查看DTM矩陣，可以發現是個稀疏矩陣。 

# 建立LDA模型
```{r}
lda <- LDA(Detention_dtm, k = 2, control = list(seed = 2020))
```

## $\phi$ Matrix

### 查看$\phi$ matrix (topic * term)
```{r}
topics <- tidy(lda, matrix = "beta") # 注意，在tidy function裡面要使用"beta"來取出Phi矩陣。
topics
```

> 從topics中可以得到特定主題生成特定詞彙的概率。

### 尋找Topic的代表字
```{r}
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```
> 透過上方的圖，感覺兩個主題看起來差不多，沒有看出兩者的差異，嘗試看看分多一點topics

## 更多主題
+ 嘗試2,3,10,25,36主題數，將結果存起來，再做進一步分析

```{r eval=FALSE}
 ldas = c()
 topics = c(2,3,10,25,36)
 for(topic in topics){
   start_time <- Sys.time()
   lda <- LDA(Detention_dtm, k = topic, control = list(seed = 2020))
   ldas =c(ldas,lda)
  print(paste(topic ,paste("topic(s) and use time is ", Sys.time() -start_time)))
   save(ldas,file = "ldas_Detentionresult")
 }

```

> 已將主題結果存在ldas_Detentionresult

### 載入每個主題的LDA結果
```{r}
load("ldas_Detentionresult")
```


### 透過perplexity找到最佳主題數
```{r}

topics = c(2,3,10,25,36)
data_frame(k = topics,
           perplex = map_dbl(ldas, topicmodels::perplexity)) %>%
  ggplot(aes(k, perplex)) +
  geom_point() +
  geom_line() +
  labs(title = "Evaluating LDA topic models",
       subtitle = "Optimal number of topics (smaller is better)",
       x = "Number of topics",
       y = "Perplexity")
```

> perplexity 越小越好，但是太小的話，主題數會分太細。通常會找一個主題數適當，且perplexity比較低的主題

# LDAvis

## 產生create LDAvis所需的json function

+ 此function是將前面使用 "LDA function"所建立的model，轉換為"LDAVis"套件的input格式。

```{r}
topicmodels_json_ldavis <- function(fitted, doc_term){
    require(LDAvis)
    require(slam)
  
   ###以下function 用來解決，主題數多會出現NA的問題
  ## 參考 https://github.com/cpsievert/LDAvis/commit/c7234d71168b1e946a361bc00593bc5c4bf8e57e
ls_LDA = function (phi)
{
  jensenShannon <- function(x, y) {
      m <- 0.5 * (x + y)
    lhs <- ifelse(x == 0, 0, x * (log(x) - log(m+1e-16)))
    rhs <- ifelse(y == 0, 0, y * (log(y) - log(m+1e-16)))
    0.5 * sum(lhs) + 0.5 * sum(rhs)
  }
  dist.mat <- proxy::dist(x = phi, method = jensenShannon)
  pca.fit <- stats::cmdscale(dist.mat, k = 2)
  data.frame(x = pca.fit[, 1], y = pca.fit[, 2])
}

    # Find required quantities
    phi <- as.matrix(posterior(fitted)$terms)
    theta <- as.matrix(posterior(fitted)$topics)
    vocab <- colnames(phi)
    term_freq <- slam::col_sums(doc_term)

    # Convert to json
    json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
                            vocab = vocab,
                            doc.length = as.vector(table(doc_term$i)),
                            term.frequency = term_freq, mds.method = ls_LDA)

    return(json_lda)
}
```


## 產生LDAvis結果

+ 可以透過 github的方式，線上demo你的LDAvis結果 範例：https://234gts868.github.io/LDA/3_ldavis/#topic=0&lambda=0.5&term=

```{r eval=FALSE}

# 設置alpha及delta參數
devotion_lda_removed <- LDA(devotion_dtm_removed, k = 4, method = "Gibbs", control = list(seed = 1234, alpha = 2, delta= 0.1))

####### 以下用來產生ldavis的檔案，可以之後用來在local端、放在網路上打開 ##########
 for(lda in ldas){
   
   k = lda@k ## lda 主題數
   if(k==2){next}
   json_res <- topicmodels_json_ldavis(lda,news_dtm)
   serVis(json_res,open.browser = T)
   lda_dir =  paste0(k,"_ldavis")
   if(!dir.exists(lda_dir)){ dir.create("./",lda_dir)}
   
   serVis(json_res, out.dir =lda_dir, open.browser = F)
   
   writeLines(iconv(readLines(paste0(lda_dir,"/lda.json")), to = "UTF8"))
 }

topic_10 = ldas[[3]]
json_res <- topicmodels_json_ldavis(topic_10,Detention_dtm)

serVis(json_res,open.browser = T)

# 如果無法開啟視窗(windows用戶)可執行這段
 serVis(json_res, out.dir = "vis", open.browser = T)
 writeLines(iconv(readLines("./vis/lda.json"), to = "UTF8"))

```


# LDA後續分析

+ 根據前面的探索之後，我們對於資料有更加了解，並且看完每個主題數的LDAvis之後，選定主題數10的結果來作後續的分析

```{r}
Detention_lda = ldas[[3]] ## 選定topic 為10 的結果

topics <- tidy(Detention_lda, matrix = "beta") # 注意，在tidyfunction裡面要使用"beta"來取出Phi矩陣。
topics
```
> 


## 尋找Topic的代表字
+ 整理出每一個Topic中生成概率最高的10個詞彙。

```{r}

top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_fill_manual(values=mycolors)+
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```
> 可以看到topic都被一開始所使用的搜尋關鍵字影響看不出每一群的差異。

+ 移除常出現、跨主題共享的詞彙。
```{r}
remove_word = c("返校","電影","自己","遊戲")
top_terms <- topics %>%
  filter(!term  %in% remove_word)%>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_fill_manual(values=mycolors)+
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```
> 可以看出每個主題主要在討論什麼了！

### 主題命名
```{r}
topic_name = c("禁書",'劇情','None1','政治、社會','None2','None3','票房','金馬獎','None4','None5')
```

## Document 主題分佈
```{r}
# for every document we have a probability distribution of its contained topics
tmResult <- posterior(Detention_lda)
doc_pro <- tmResult$topics 
dim(doc_pro)               # nDocs(DTM) distributions over K topics
```
> 每篇文章都有topic的分佈，所以613筆的文章*10個主題


### cbind Document 主題分佈
```{r}
# get document topic proportions 
document_topics <- doc_pro[Detention$artUrl,]
document_topics_df =data.frame(document_topics)
colnames(document_topics_df) = topic_name
rownames(document_topics_df) = NULL
news_topic = cbind(Detention,document_topics_df)
```

> 現在我們看每一篇的文章分佈了！

### 查看特定主題的文章
+ 透過找到特定文章的分佈進行排序之後，可以看到此主題的比重高的文章在討論什麼。

```{r ,eval=FALSE}
 news_topic %>%
    arrange(desc(`禁書`)) %>% head(10) 
```

> 可以看到禁書這個主題主要是在：探討歷史上禁書、政府打壓文論自由的議題

### 了解主題在時間的變化
```{r warning=FALSE}
# news_topic[,c(11:20)] =sapply(news_topic[,c(11:20)] , as.numeric) #這行會error, 還沒找到解法
news_topic %>% 
  group_by(artDate = format(artDate,'%Y%m')) %>%
  summarise_if(is.numeric, sum, na.rm = TRUE) %>%
  melt(id.vars = "artDate")%>%
 ggplot( aes(x=artDate, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("value") + 
  scale_fill_manual(values=mycolors)+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

> 移除資料較少的月份，以及None的主題


#### 去除筆數少月份、及None的主題
```{r warning=FALSE}
news_topic %>%
  filter( !format(artDate,'%Y%m') %in% c(201912,202002,202003,202004))%>%
  dplyr::select(-None1,-commentNum,-push,-boo,-None2,-None3,-None4,-None5)%>%
  group_by(artDate = format(artDate,'%Y%m')) %>%
  summarise_if(is.numeric, sum, na.rm = TRUE) %>%
  melt(id.vars = "artDate")%>%
 ggplot( aes(x=artDate, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("value") + 
    scale_fill_manual(values=mycolors)+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

> 可以看出每個月的聲量，但是不能很清楚出每個月的比例

#### 以比例了解主題時間變化
```{r warning=FALSE}
news_topic %>%
  filter( !format(artDate,'%Y%m') %in% c(201912,202002,202003,202004))%>%
  dplyr::select(-None1,-commentNum,-push,-boo,-None2,-None3,-None4,-None5)%>%
  group_by(artDate = format(artDate,'%Y%m')) %>%
  summarise_if(is.numeric, sum, na.rm = TRUE) %>%
melt(id.vars = "artDate")%>%
  group_by(artDate)%>%
  mutate(total_value =sum(value))%>%
 ggplot( aes(x=artDate, y=value/total_value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
      scale_fill_manual(values=mycolors)+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


